{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4 - Обучение с подкреплением (reinforcement learning)\n",
    "\n",
    "**Обучение с подкреплением** - один из способов машинного обучения, где некоторый агент обучается, взаимодействуя со средой. В отличие от обучения с учителем тут нет размеченных данных. Однако агент обладает набором допустимых действий, выполняя которые, он получает награду. Цель агента - максимизировать суммарную награду за какой-то период времени. Рассмотрим возможные постановки задачи.\n",
    "\n",
    "Материал этой лабораторной основывается на этой прекрасной [книге](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf), которой вы можете впоспользоваться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Задача о многоруком бандите (multi-armed bandit problem)\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "#### Неформальная\n",
    "Пусть есть $N$ игровых автоматов. Сыграв на $i$-м автомате, агент получает награду, которая задается случайной величиной. Агент не знает, как устроены распределения, однако перед ним стоит задача максимизировать выигрыш, сыграв $T$ раз.\n",
    "\n",
    "#### Формальная\n",
    "Пусть есть конечное множество возможных действий $\\mathcal{A}$. Для каждого действия $a \\in \\mathcal{A}$ существует премия, которая определяется неизвестным распределением $p(r|a)$. Стратегия агента (policy) в момент $t$ - это некоторое вероятностное распределение на множестве всех действий $\\pi_t(a)$.\n",
    "\n",
    "Игра происходит следующим образом:\n",
    "> 1. У агента есть некоторая начальная стратегия $\\pi_1(a)$\n",
    "> 2. В каждый момент времени $1 \\leq t \\leq T$:\n",
    "> 3. Выбирает действией $a_t \\sim \\pi_t(a)$\n",
    "> 4. Получает свою награду $r_t \\sim p(r|a_t)$\n",
    "> 5. Корректирует свою стратегию $\\pi_t \\rightarrow \\pi_{t+1}$\n",
    "\n",
    "Пусть $c_t(a)$ - количество раз, которое агент выбрал действие $a$ к моменту $t$\n",
    "$$c_t(a) = \\sum_{i=1}^{t}[A_i = a]$$\n",
    "Тогда задачей агента является минимизация сожаления к моменту $T$, а именно\n",
    "$$T\\cdot\\mu^* - \\sum_a \\mu_a \\mathbb{E}[c_T(a)]$$\n",
    "* $\\mu_a$ - мат. ожидание награды за действие $a$\n",
    "* $\\mu^* = \\max_a \\mu_a$ - мат. ожидание оптимального действия\n",
    "\n",
    "Также можно встретить следующее определение сожаления:\n",
    "$$T\\cdot\\mu^* - \\mathbb{E}\\big[ \\sum_{t=1}^{T} R_t \\big].$$\n",
    "\n",
    "Впервые задача была предложена в этой [статье](http://projecteuclid.org/download/pdf_1/euclid.bams/1183517370).\n",
    "### Модельная задача\n",
    "1. $|A| = 100, T = 1000$\n",
    "2. $\\mu_a \\sim \\mathcal{N}(0, 1)$\n",
    "3. $p(r|a) = \\mathcal{N}(r; \\mu_a, 1)$\n",
    "\n",
    "Симуляция игры запускается $10^4$ раз, и строится график в осях **[номер шага в игре]** $\\times$ **[усредненная суммарная премия к текущему шагу]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Известные стратегии\n",
    "Введем следущие определения:\n",
    "* $Q_t(a)$ - средняя премия действия $a$ к раунду $t$ (value of action), при росте $c_t(a)$ стремится к $\\mu_a$.\n",
    "* $Q^*(a) = \\lim_{t \\rightarrow \\infty} Q_t(a)$ - ценность действия $a$.\n",
    "* $\\mathcal{A}_t = \\arg\\max_a Q_t(a)$ - множество действий, которое имеет максимальную среднюю премию к раунду $t$.\n",
    "\n",
    "**Задание**\n",
    "1. Можно ли вычислить $Q_{t+1}(a)$ инкрементально (известно лишь  $Q_t(a)$ и награда  $r_{t+1}$, назначенная за выбор действия $a$)?\n",
    "2. Используйте этот подход далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Жадная стратегия (greedy policy)\n",
    "Стратегия $\\pi_t$ заключается в том, что мы равновероятно выбираем $a$ из $\\mathcal{A}_t$\n",
    "$$\\pi_t(a)= \\frac{1}{|\\mathcal{A}_t|}[a \\in \\mathcal{A}_t]$$\n",
    "**Задание**\n",
    "1. Реализуйте данную стратегию\n",
    "2. Опишите ее главный недостаток\n",
    "3. Как вы инициализировали $\\pi_1$?\n",
    "4. Нужно ли несколько стартовых игр, чтобы инициализировать $\\pi_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\varepsilon$-жадная стратегия ($\\varepsilon$-greedy policy)\n",
    "Проблема предыдущего подхода в том, что он лишь жадным образом пытается эксплуатировать среду. Однако какое-то время необходимо тратить также на ее изучение, чтобы максимизировать премию в долгосрочном периоде. Таким образом идет речь о балансе *exploration/exploitation*.\n",
    "\n",
    "Возможная модификация предыдущего подхода:\n",
    "$$\\pi_t(a)=\\frac{1-\\varepsilon}{|\\mathcal{A}_t|}[a \\in A_t] + \\frac{\\varepsilon}{|\\mathcal{A}|},\\ \\varepsilon \\in [0, 1]$$\n",
    "\n",
    "\n",
    "\n",
    "**Задание**\n",
    "1. Реализуйте данную стратегию\n",
    "2. Что происходит с ростом $\\varepsilon$?\n",
    "3. Как бы изменяли $\\varepsilon$ по мере обучения агента?\n",
    "4. Опробуйте жадную и $\\varepsilon$-жадную стратегии на модельной задаче. Попробуйте разные $\\varepsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "Другая интерпретация $\\varepsilon$-жадной стратегии\n",
    "$$\\pi_t(a)=\\frac{\\exp(\\frac{1}{\\varepsilon} \\cdot Q_t(a))}{\\sum_{b} \\exp(\\frac{1}{\\varepsilon} \\cdot Q_t(b))},\\ \\varepsilon > 0$$\n",
    "\n",
    "**Задание**\n",
    "1. Реализуйте данную стратегию\n",
    "2. Что происходит, если $\\varepsilon$ стремится к $0$? А бесконечности?\n",
    "3. Сравните softmax и $\\varepsilon$-жадную стратегии на модельной задаче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод UCB (upper confidence bound)\n",
    "Предложенный ниже алгоритм в некотором смысле является оптимальным, подробнее об этом [здесь]( http://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf). Для каждого момента $t$ он определяет множество наиболее потенциально выгодных действий. Выбор $a$ из $A_t$ происходит равновероятно.\n",
    "\n",
    "$$\\mathcal{A}_t = \\arg\\max Q_t(a) + \\varepsilon \\sqrt{\\frac{2 \\ln t}{c_t(a)}},\\ \\varepsilon \\geq 0$$\n",
    "\n",
    "Первая часть нам уже знакома, а вот вторую часть можно воспринимать, как величину, показывающую, насколько точна наша оценка $Q_t(a)$. Стратегия сама пытается найти баланс между *exploration/exploitation*.\n",
    "\n",
    "**Задание**\n",
    "1. Реализуйте данную стратегию\n",
    "2. Что происходит, если $\\varepsilon$ увеличивается?\n",
    "3. На модельной задаче сравните лучшую версию $softmax$, $\\varepsilon$-жадную стратегии и UCB метод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный метод (gradient bandit policy)\n",
    "\n",
    "Существуют еще так называемые адаптивные стратегии. Они могут быть использованы, если среда не является стационарной (распределение премий может меняться). В таком случае предлагается использовать уже знакомое нам экспоненциальное сглаживание.\n",
    "\n",
    "Мы подсчитываем среднюю сглаженную премию к моменту $t$ по всем действиям ($\\alpha$ регулирует глубину истории):\n",
    "$$\\bar{r}_{t+1} = (1-\\alpha_t)\\cdot\\bar{r}_t(a)+\\alpha_t r_{t+1} = \\bar{r}_t +\\alpha_t (r_{t+1}-\\bar{r}_t(a)),\\ \\alpha_t \\in [0, 1]$$\n",
    "\n",
    "**Замечание**\n",
    "Нужно все же сказать о сходимости подобных вещей, условие следующее:\n",
    "* $\\sum^{\\infty} \\alpha_t = \\infty$\n",
    "* $\\sum^{\\infty} \\alpha^2_t < \\infty$\n",
    "\n",
    "Кстати, если взять  $\\alpha_t = \\frac{1}{t}$, то получится просто среднее значение всех $r$. Это подсказка на самое первое задание.\n",
    "\n",
    "Для каждого действия у нас есть приоритет $p_t(a)$. После очередного шага идет корректировка с шагом $\\lambda$. Если было выбрано действие $a$, то\n",
    "$$p_{t+1}(a) = p_t(a)+\\lambda(r_t-\\bar{r}_t)(1-\\pi_t(a))$$\n",
    "в ином случае\n",
    "$$p_{t+1}(a) = p_t(a)-\\lambda(r_t-\\bar{r}_t)\\pi_t(a)$$\n",
    "\n",
    "Тогда стратегия на момент $\\pi_{t+1}$ будет выглядеть так:\n",
    "$$\\pi_{t+1}(a) = \\frac{\\exp(p_{t+1}(a))}{\\sum_{b} \\exp(p_{t+1}(b))}$$\n",
    "Подробное обоснование метода вы можете найти в книге.\n",
    "\n",
    "\n",
    "**Задание**\n",
    "1. Реализуйте данную стратегию.\n",
    "2. Покажите, какая из описанных моделей ведет себя лучше на модельной задаче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дальнейшее чтение\n",
    "Существует также задача многорукого бандита с контекстом (contextual bandits), формальная постановка выглядит следующим образом.\n",
    "* $\\mathcal{A}$ - множество допустимых действий\n",
    "* $X$ - пространство контекстов среды\n",
    "* $p(r|a, x)$ - распределение премии для действия $a$ в условиях контекста $x$\n",
    "* $\\pi_t(a|x)$ - стратегия агента на момент $t$ в условиях контекста $x$.\n",
    "\n",
    "Пример использования такой модели при показе новостей [здесь](http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Марковский процесс принятия решения (markov decison process)\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "Пусть есть среда с конечным множество состояний $\\mathcal{S}$. В момент времени $t$ агент, находясь в состоянии $s_t$, может выбрать произвольное действие $a_t \\in \\mathcal{A}_{s_t} \\subset \\mathcal{A}$. После чего среда переходит в состояние $s_{t+1}$ и агент получает награду $r_{t+1}$. Поведение среды задается распределением\n",
    "\n",
    "$$p(s_{t+1}, r_{t+1}|s_t, a_t)  = \\Pr\\{ S_{t+1} = s_{t+1}, R_{t+1} =  r_{t+1} | S_t = s_t, A_t = a_t\\}.$$\n",
    "\n",
    "Агент обладает некоторой стратегией $\\pi$, которая на шаге $t$ задает вероятности выбора действия $a_t \\in \\mathcal{A}_{s_t}$ для состояния $s_t$ $\\pi_t(a_t|s_t) = p(a_t|s_t)$.\n",
    "\n",
    "Неформально задача агента - максимизировать суммарную награду. Если процесс принятия решения конечный, то ее можно просто формализовать, как максимизацию величины\n",
    "$$G = \\sum_{t=0}^{T} R_{t+1},$$\n",
    "\n",
    "где $T$ - момент времени, когда мы попали в какое-то конечное (терминальное) состояние, в данном контексте стоит воспринимать, как случайную величину. Если же процесс может длиться очень долго или бесконечно, то мы имем дело с диксконтированной  наградой\n",
    "$$ G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1},$$\n",
    "здесь $\\gamma \\in [0, 1]$ определяет на сколько далеко по времени мы смотрим в будущее.\n",
    "\n",
    "**Замечания**\n",
    "\n",
    "1. Если предположить, что попав в конечное состояние, то мы вегда остаемся в нем и в дальнейшем получаем только нулевую награду, то положив $\\gamma = 1$, можно получить первый вариант определения.\n",
    "2. \"Марковость\" процесса заключается в том, что среда изменяется и выдает награду только на основе текущего состояния и последнего действия агента.\n",
    "\n",
    "**Задание**\n",
    "1. Привидите жизненный пример марковского процесса принятия решения (это может быть какаю-нибудь игра и т.п.).\n",
    "2. Можете ли вы привести пример игры, где принятие решения нельзя смоделировать с помощью марковского процесса?\n",
    "3. Выведите следующие значения через $p(s_{t+1}, r_{t+1}|s_t, a_t)$, для простоты все распределения можно считать дискретными\n",
    "  * $r(s_{t}, a_{t}) = \\mathbb{E}[R_{t+1}|S_t = s_t, A_t = a_t]$ - средняя награда за действие $a_t$ в $s_t$ \n",
    "  * $p(s_{t+1} | s_t, a_t) = \\Pr\\{S_{t+1} = s_{t+1} | S_t = s_t, A_t = a_t \\}$ - вероятность попасть в $s_{t+1}$ из $s_t$, сделав $a_t$.\n",
    "  * $r(s_t, a_t, s_{t+1}) = \\mathbb{E}[R_{t+1}|S_{t+1} = s_{t+1}, S_t = s_t, A_t = a_t]$ - средняя награда при переезде из $s_t$ в $s_{t+1}$, сделав $a_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные понятия\n",
    "\n",
    "Ценность состояния (state-value function) при стратегии $\\pi$, это награда, которую мы получим в дальнешем, стартуя из него\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}[G_t|S_t=s] = \\mathbb{E}\\Big[\\sum_{k=0}^{\\infty} \\gamma^{k+1}R_{t+k+1}|S_t=s\\Big].$$\n",
    "\n",
    "Ценности действия (action-value function) $a$ в состоянии $s$, это также дальнейшая награда, которую мы получим при стратегии $\\pi$\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}[G_t|S_t=s, A_t=a] = \\mathbb{E}\\Big[\\sum_{k=0}^{\\infty} \\gamma^{k+1}R_{t+k+1}|S_t=s,  A_t=a\\Big].$$\n",
    "\n",
    "Прим этом верны следующие тождества:\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}\\big[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s\\big]$$\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\mathbb{E}\\big[R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t=a\\big]$$\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\mathbb{E}\\big[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s, A_t=a\\big]$$\n",
    "\n",
    "Первы два являются уравнениямит Беллмана (Bellman equation).\n",
    "\n",
    "**Замечание**\n",
    "Важно отметить, в этих определениях стратегия $\\pi$ фиксирована и не меняется со временем. Хотя очевидно, что на реальной практике это не так, потому что наша стратегия постоянно дообучается по мере изучения среды.\n",
    "\n",
    "Мы будем считать стратегию $\\pi^{*}$ оптимальной, если для любого состояния $s$ и любой другой стратегии $\\pi$ верно $V_{\\pi^{*}}(s) \\geq V_{\\pi}(s)$. Таких стратегий может быть несколько, однако функции $V(s)$ и $Q(s, a)$, которые они определяют, будут одинаковыми. Эти функции мы и обозначим, как оптимальные $V^*(s)$ и $Q^*(s, a)$.\n",
    "\n",
    "Прим этом существует уравнение оптимальности Беллмана (Bellman optimality equation), подробнее [здесь](https://en.wikipedia.org/wiki/Bellman_equation):\n",
    "\n",
    "$$V^*(s) = \\max_{a \\in \\mathcal{A}} \\mathbb{E}\\big[R_{t+1} + \\gamma V^*(S_{t+1}) | S_t=s, A_t=a\\big]$$\n",
    "\n",
    "$$Q^*(s, a) = \\mathbb{E}\\big[R_{t+1} + \\gamma \\max_{b \\in \\mathcal{A}} Q^*(S_{t+1}, b) | S_t=s, A_t=a\\big]$$\n",
    "\n",
    "Тогда следующие две жадные стратегии являются эквивалентными и оптимальными ($\\mathcal{A}_t$ - множество):\n",
    "$$\\mathcal{A}_t = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E}\\big[R_{t+1} + \\gamma V^*(s_{t+1}) | s_t, a \\big]$$\n",
    "$$\\mathcal{A}_t = \\arg\\max_{a \\in \\mathcal{A}} Q^*(s_t, a)$$\n",
    "\n",
    "### Динамическое программирование (Dynamic programming)\n",
    "\n",
    "Материал этой части строится на том, что $p(s',r|s,a)$ известна. Это пригодится нам для дальнейшего понимания.\n",
    "#### Вычисление $V^{\\pi}(s)$ (iterative policy evaluation)\n",
    "\n",
    "Пусть есть некоторая фиксированная стратегия $\\pi$, тогда для функции $V^{\\pi}$ справедлива рекуррентная формула\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[r + \\gamma V^{\\pi}(s') \\big]$$\n",
    "\n",
    "Для простоты записи формул мы предполагаем, что распределение наград является дискретной случайной величиной. В ином случае необходимо использовать оператор интегрирования вместо суммы. Таким образом мы поможем вычислить функцию $V^{\\pi}$ для каждого состояния $s$ используя итерационный алгоритм, инициализировав $V_0$ случайно\n",
    "\n",
    "$$V_{n+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[r + \\gamma V_n(s') \\big]$$\n",
    "\n",
    "Кстати, если есть конечные состояния, то для них  $V(s)$ уже известно и эти значения стоит использовать при инициализации для более быстрой сходимости. Вычисления останавливаются, когда изменение функции $V(s)$ от итерации к итерации стало незначительным.\n",
    "\n",
    "**Замечание**\n",
    "1. Мы делаем предположение, что $p(s',r|s,a)$ нам известно, что на практике частно не так.\n",
    "2. Вычисления $V_{n+1}$ можно делать *in place*, что только ускоряет сходимость, так как в процессе вычисления мы будем использовать, как значения из итерации $n$ так и какие-то новые значения из итерации $n+1$.\n",
    "\n",
    "#### Улучшения стратегии $\\pi$ (policy improvement theorem)\n",
    "Пусть есть две стратегии $\\pi$ и $\\pi'$, при этом известно, что для любого состояния $s$ верно, что \n",
    "$$Q^{\\pi}(s, \\pi'(s)) \\geq V^{\\pi}(s),$$\n",
    "то тогда справедливо неравенство\n",
    "$$V^{\\pi'}(s) \\geq V^{\\pi}(s).$$\n",
    "\n",
    "Весь интерес данной теоремы заключается в том, что она позволяет нам улучшать текущую стратегию. Мы умеем вычислять $V^{\\pi}(s)$, тогда \n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) =\n",
    "\\mathbb{E}\\big[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s, A_t=a\\big] =\n",
    "\\sum_{s',r}p(s',r|s, a)\\big[ r + \\gamma V^{\\pi}(s') \\big]\n",
    "$$\n",
    "\n",
    "Это позволяет нам улучшать стратегию\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a Q^{\\pi}(s, a).$$\n",
    "Эта запись означает, что мы распределяем вероятность между множеством состояний, вычисленных в правой части. Если мы не можем улучить $V^{\\pi'}$ относительно $V^{\\pi}$, то значит функция ценности состояния оптимальна, так как удовлетвояют уравнению Беллмана, а значит оптимальна и стратегия.\n",
    "\n",
    "#### Метод итерации стратегий (policy interation) \n",
    "Таким образом, совершая поочередно вычисление $V^{\\pi}$ и улушчение $\\pi$, найдем $\\pi^*$. Обычный итеративный алгоритм с двумя этапами. Стоити отметить, что в качестве инициализции значений функции $V(s)$ можно использовать значения для предыдущей стратегии, это значительно ускоряет сходимость алгоритма.\n",
    "\n",
    "#### Метод итерации ценности состояний (value interation) \n",
    "Также существует достаточно интересный подход для вычисления оптимальной $V^{*}(s)$, а значит и нахождения оптимальной стратегии.\n",
    "\n",
    "$$V_{n+1}(s) = \\max_a \\sum_{s',r} p(s',r|s,a) \\big[r + \\gamma V_n(s') \\big].$$\n",
    "\n",
    "Относится к этому алгоритму можно по разному. Во-первых, его можно воспринимать, как то, что в методе итерации стратегий при вычисление $V^{\\pi}$ мы не ждем сходимости, делаем одну итерацию, а затем сразу подправляем стратегию. С другой стороны по сути мы выписали уравнение оптимальности Беллмана и используем его в качестве итеративного алгоритма. К этому методу стоит относится именно, как к практическому.\n",
    "\n",
    "**Задание**\n",
    "1. Смоделируйте некоторую среду. Пусть будет 100 состояний. Распределение функции $p(s',r|s,a)$ можете взять на свое усмотрение.\n",
    "2. Зафиксируйте некоторе значение дисконтирования. Найдите лучшую стратегию.\n",
    "3. Сравните сходимость метода итерации стратегий и итерации ценности состояний.\n",
    "4. Выберете наиболее быстрый метод. Посмотрите, как меняется значение функции $V^*(s)$ в зависимоти от $\\gamma$.\n",
    "5. Постройте график $\\gamma \\times V^*(s)$ для 5 произвольных состояний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Модельная задача\n",
    "В качестве модельной задачи выберем простые крестики-нолики. Существует два агента, один играет за $X$s, другой за $O$s. Один агент играет против другого и по сути являетя средой для другого игрока. Множество $\\mathcal{S}_{X}$ - всевозможные допустимые расстановки крестиков после хода нолика и $\\mathcal{S}_{O}$ - позиции после хода крестиков. Необходимо обучить обоих агентов в ходе серии игр друг с другом. Критерий достижения успеха - оба агента вам не проигрывают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Метод временных разностей (Temporal difference)\n",
    "Речь ниже пойдет уже о более современных методах, которые можно применять на практике. Представим, что мы вычисляем функцию $V^{\\pi}(s)$ для нашей стратегии $\\pi$. Пусть мы очередной раз посетили состояние $s_t$, а потом каким-то образом узнали итоговую наград $G_t$. Тогда постепенно посещая состояние $s_t$ раз за разом, используя экспоненциальное сглаживание, мы можем улучшать оценку $V^{\\pi}$ для какой-то нашей стратегии $\\pi$.\n",
    "\n",
    "$$V^{\\pi}(s_t) = V^{\\pi}(s_t) + \\alpha \\big(G_t - V^{\\pi}(s)\\big)$$\n",
    "\n",
    "Однако можно сделать небольшой трюк и расписать оценку $G_t$\n",
    "$$V^{\\pi}(s_t) = V^{\\pi}(s_t) + \\alpha \\big(R_t + \\gamma V^{\\pi}(S_t) - V^{\\pi}(s)\\big),$$\n",
    "тем самым мы получим формулы для итеративного вычисления функции $V^{\\pi}(s)$ после каждого посещения состояния $s$. Аналогичным образомы мы можем вычислять и \n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) - Q^{\\pi}(s_t, a_t)\\big).$$\n",
    "\n",
    "#### SARSA (State-Action-Reward-State-Action)\n",
    "Итак, мы хотим найти оптимальную стратегию $\\pi^{*}$. Пусть у нас есть некоторая стартовая апроксимация $Q(s, a)$. На основе нее мы создаем $\\varepsilon$ жадную стратегию $\\mathcal{A}_s = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)$, тогда\n",
    "\n",
    "$$\\pi(a|s)=\\frac{1-\\varepsilon}{|\\mathcal{A}_s|}[a \\in \\mathcal{A}_s] + \\frac{\\varepsilon}{|\\mathcal{A}|},\\ \\varepsilon \\in [0, 1].$$\n",
    "\n",
    "После этого в соответствии со стратегией $\\pi$ делаем шаг $a_t$ получаем награду $R_{t+1}$ и переходим в состояние $S_{t+1}$. Еще раз делаем выбор действия $A_{t+1} \\sim \\pi(a|S_{t+1})$, но не говорим его среде, а лишь используем для обновления $Q$\n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) - Q^{\\pi}(s_t, a_t)\\big).$$\n",
    "\n",
    "При обновлении $Q$ так же есть средняя вариация, вы можете использовать среднее значение для оценки дальнейшего выигрыша\n",
    "\n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma \\sum_b \\pi(b|S_{t+1}) Q^{\\pi}(S_{t+1}, b) - Q^{\\pi}(s_t, a_t)\\big).$$\n",
    "\n",
    "####  Q-обучение (Q-learning)\n",
    "Идея этого подхода очень похожа, только жадность также применяется в момент обновления  функции $Q$ \n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma \\max_{b} Q(S_{t+1}, b) - Q(s_t, a_t)\\big).$$\n",
    "\n",
    "**Задание**\n",
    "1. Реализуйте оба алгоритма в рамках модельной задачи.\n",
    "2. Оцените, какой из них быстрее сходится, как бы вы это стали делать?\n",
    "3. Сравните на сколько сильно отличаются оценки $Q(s_t, a_t)$. Предложите свой способ.\n",
    "4. Попробуйте сыграть со своим алгоритма. Используйте намеренно проигрышную стратегию за ноликов. Что произошло? Попробуйте обыграть агента за крестиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
