{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# L4 - Reinforcement learning\n",
    "\n",
    "**Reinforcement learning** is one of the machine learning approaches, where our agent learns by interacting with the some environment. In contrast to the supervised learning there are no labeled objects. However, agent can get rewards, making some actions. The goal of the agent is to maximize the total reward over some time period. \n",
    "\n",
    "This lab is mainly based on the [book](http://incompleteideas.net/book/bookdraft2018jan1.pdf). You just must read it, if you are interesting in reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Multi-armed bandit problem\n",
    "\n",
    "#### Problem statement\n",
    "##### Informal\n",
    "Let we have $N$ slot machines. Playing on the $i$th slot, the agent receives some random reward. The agent doesn't know probability distribution of rewards, however it needs to maximize winnings after $T$ games.\n",
    "\n",
    "##### Formal\n",
    "Let $\\mathcal{A}$ be a finite set of possible actions. For each action $a \\in \\mathcal{A}$ agent can get some reward which is determined by the unknown probability distribution $p(r|a)$. The strategy of the agent (policy) at time $t$ is some probability distribution on the set of actions – $\\pi_t(a)$.\n",
    "\n",
    "Games' steps:\n",
    "> 1. Agent has some initial policy $\\pi_1(a)$\n",
    "> 2. At time  $1 \\leq t \\leq T$:\n",
    "> 3. Agent randomly chooses action $a_t \\sim \\pi_t(a)$\n",
    "> 4. Gets some reward $r_t \\sim p(r|a_t)$\n",
    "> 5. And update its policy $\\pi_t \\rightarrow \\pi_{t+1}$\n",
    "\n",
    "Let $c_t(a)$ be how many times action $a$ was used at  moment $t$\n",
    "$$c_t(a) = \\sum_{i=1}^{t}[A_i = a].$$\n",
    "Then agent needs to minimize its regret\n",
    "$$T\\cdot\\mu^* - \\sum_a \\mu_a \\mathbb{E}[c_T(a)]$$\n",
    "* $\\mu_a$ – expected value of reward for action $a$\n",
    "* $\\mu^* = \\max_a \\mu_a$ – expected value of reward for optimal action\n",
    "\n",
    "So we can define regrest as following\n",
    "$$T\\cdot\\mu^* - \\mathbb{E}\\big[ \\sum_{t=1}^{T} R_t \\big].$$\n",
    "\n",
    "The first task was formulated in this [paper](http://projecteuclid.org/download/pdf_1/euclid.bams/1183517370).\n",
    "#### Model problem\n",
    "1. $|A| = 100, T = 1000$\n",
    "2. $\\mu_a \\sim \\mathcal{N}(0, 1)$\n",
    "3. $p(r|a) = \\mathcal{N}(r; \\mu_a, 1)$\n",
    "\n",
    "The game simulated $10^4$ times and we are interesting in plot **[step]** $\\times$ **[average total reward at current step]**.\n",
    "\n",
    "1. $Q_t(a)$ – average reward of action $a$ by moment $t$ (value of action), $\\lim_{c_t(a) \\rightarrow \\infty} Q_t(a) =\\mu_a$.\n",
    "2. $Q^*(a) = \\lim_{t \\rightarrow \\infty} Q_t(a) = \\mu_a$ – value of cation $a$.\n",
    "3. $\\mathcal{A}_t = \\arg\\max_a Q_t(a)$ – set of actions that has the greatest average reward at moment $t$.\n",
    "\n",
    "#### Greedy policy\n",
    "$$\\pi_t(a)= \\frac{1}{|\\mathcal{A}_t|}[a \\in \\mathcal{A}_t].$$\n",
    "\n",
    "#### Exercises\n",
    "1. Implement gready policy.\n",
    "2. What is main disadvantages?\n",
    "3. How do you init $\\pi$? Is there any better option?\n",
    "4. Check this policy on model problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### $\\varepsilon$-greedy policy\n",
    "The previous approach only exploits the environment. However, agent should spend some time learning probability distribtion of rewards, to maximize its reward in the long term. Thus is a question of balance **exploration** and **exploitation**.\n",
    "\n",
    "Possible improvement:\n",
    "$$\\pi_t(a)=\\frac{1-\\varepsilon}{|\\mathcal{A}_t|}[a \\in A_t] + \\frac{\\varepsilon}{|\\mathcal{A}|},\\ \\varepsilon \\in [0, 1].$$\n",
    "\n",
    "#### Exercises\n",
    "1. Implement this policy.\n",
    "2. What happens if you increase $\\varepsilon$?\n",
    "3. How do you change $\\varepsilon$ during agent learning?\n",
    "4. Check this policy on model problem, using different $\\varepsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Softmax\n",
    "One more interpretation of $\\varepsilon$-greedy policy\n",
    "\n",
    "$$\\pi_t(a)=\\frac{\\exp(\\frac{1}{\\varepsilon} \\cdot Q_t(a))}{\\sum_{b} \\exp(\\frac{1}{\\varepsilon} \\cdot Q_t(b))},\\ \\varepsilon >0.$$\n",
    "\n",
    "#### Exercises\n",
    "1. Implement this policy.\n",
    "2. What happens if $\\varepsilon \\rightarrow 0$? What about $\\varepsilon \\rightarrow \\infty$?\n",
    "3. Compare softmax and $\\varepsilon$-greedy policy on model problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### UCB (upper confidence bound)\n",
    "For each moment $t$ agent defines the most potentially profitable actions. Agent choice of $a \\in A_t$ in accordance with following rule\n",
    "$$\\mathcal{A}_t = \\arg\\max Q_t(a) + \\varepsilon \\sqrt{\\frac{2 \\ln t}{c_t(a)}},\\ \\varepsilon \\geq 0$$\n",
    "\n",
    "The first part is well known, but the second summand is measure of how accurate agent estimates $Q_t(a)$ at this moment. The policy itself balances between exploration and exploitation. This policy can be considired as optimal in some sense, more inforamtion [here](http://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf).\n",
    "\n",
    "#### Exercises\n",
    "1. Implement policy.\n",
    "2. What happens if $\\varepsilon \\rightarrow 0$? What about $\\varepsilon \\rightarrow \\infty$?\n",
    "3. Compare this policy with softmax and $\\varepsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Gradient policy\n",
    "There are also adaptive policy. They can be used, if the environment is not stationary (the distribution of rewards may slowly change). In this case, it is proposed to use the already familiar exponential smoothing\n",
    "We find smoothed average reward at moment $t$ for all actions:\n",
    "$$\\bar{r}_{t+1} = (1-\\alpha_t)\\cdot\\bar{r}_t(a)+\\alpha_t r_{t+1} = \\bar{r}_t +\\alpha_t (r_{t+1}-\\bar{r}_t(a)),\\ \\alpha_t \\in [0, 1].$$\n",
    "\n",
    "**Hint** Сonvergence condition\n",
    "* $\\sum^{\\infty} \\alpha_t = \\infty$\n",
    "* $\\sum^{\\infty} \\alpha^2_t < \\infty$\n",
    "\n",
    "By the way, if $\\alpha_t = \\frac{1}{t}$, then $\\bar{r}_{t+1}$ is just average of all $r_t$.\n",
    "\n",
    "For each action we have some priority $p_t(a)$. After each step we make following update\n",
    "$$p_{t+1}(a) = p_t(a)+\\lambda(r_t-\\bar{r}_t)(1-\\pi_t(a)), \\text{ $a$ is choosen,}$$\n",
    "\n",
    "$$p_{t+1}(a) = p_t(a)-\\lambda(r_t-\\bar{r}_t)\\pi_t(a), \\text{ another case.}$$\n",
    "\n",
    "So policy $\\pi_{t+1}$ looks like:\n",
    "$$\\pi_{t+1}(a) = \\frac{\\exp(p_{t+1}(a))}{\\sum_{b} \\exp(p_{t+1}(b))}$$\n",
    "More details about this method you can find in the book.\n",
    "\n",
    "#### Exercise\n",
    "1. Implement policy.\n",
    "2. In conclusion, try to understand which model is better for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Markov decison process\n",
    "#### Problem statement\n",
    "Now agent acts in some environment with finite set of states $\\mathcal{S}$. At moment $t$ agent at state $s_t$ can choose action $a_t \\in \\mathcal{A}_{s_t} \\subset \\mathcal{A}$. After agent's action, environment moves it to state $s_{t+1}$ and give reward $r_{t+1}$. Environment behaviour is defined with\n",
    "$$p(s_{t+1}, r_{t+1}|s_t, a_t)  = \\Pr\\{ S_{t+1} = s_{t+1}, R_{t+1} =  r_{t+1} | S_t = s_t, A_t = a_t\\}.$$\n",
    "\n",
    "Agent has some policy $\\pi$, which at moment $t$ defines probabilty of action $a_t \\in \\mathcal{A}_{s_t}$ for state $s_t$ $\\pi_t(a_t|s_t) = p(a_t|s_t)$. Informally agent needs to maximize the total reward. If the decision process is finite, the problem is formalized as maximizing\n",
    "$$G = \\sum_{t=0}^{T} R_{t+1},$$\n",
    "here $T$ – moment of time, when agent achives final state. We will consider this value as a random. If the process can last very long or infinite time than we have dealing with a discounted reward\n",
    "$$ G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1},$$\n",
    "here $\\gamma \\in [0, 1]$ defines how deep we look into the future.\n",
    "\n",
    "The probability that the process moves into its new state is influenced by the chosen action. Specifically, it is given by the state transition function. Thus, the next state  depends on the current state and the decision maker's action. But given state and action, it is conditionally independent of all previous states and actions, in other words, the state transitions of an MDP satisfies the Markov property.\n",
    "\n",
    "#### Exercises\n",
    "1. Give life example of Markov decision process (it may be any game, etc.).\n",
    "2. Can you give an example where a decision can't be modeled with a Markov process?\n",
    "3. Using $p(s_{t+1}, r_{t+1}|s_t, a_t)$, find values below (all distribution are discrete):\n",
    "  * $r(s_{t}, a_{t}) = \\mathbb{E}[R_{t+1}|S_t = s_t, A_t = a_t]$ – average reward for action $a_t$ in state $s_t$ \n",
    "  * $p(s_{t+1} | s_t, a_t) = \\Pr\\{S_{t+1} = s_{t+1} | S_t = s_t, A_t = a_t \\}$ – probability to move from $s_t$ to $s_{t+1}$, making $a_t$.\n",
    "  * $r(s_t, a_t, s_{t+1}) = \\mathbb{E}[R_{t+1}|S_{t+1} = s_{t+1}, S_t = s_t, A_t = a_t]$ – average reward when moving from $s_t$ to $s_{t+1}$, making $a_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Main definitions \n",
    "State-value function under the strategy of $\\pi$ returns for state $s$ total reward, that agent gets, starting from $s$\n",
    "$$V^\\pi(s) = \\mathbb{E}[G_t|S_t=s] = \\mathbb{E}\\Big[\\sum_{k=0}^{\\infty} \\gamma^{k+1}R_{t+k+1}|S_t=s\\Big].$$\n",
    "\n",
    "Action-value function under the strategy $\\pi$ for action $a$ and state $s$ returns total reward that agent gets, starting from $s$ and making $a$\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}[G_t|S_t=s, A_t=a] = \\mathbb{E}\\Big[\\sum_{k=0}^{\\infty} \\gamma^{k+1}R_{t+k+1}|S_t=s,  A_t=a\\Big].$$\n",
    "\n",
    "Thus we have following equations (the first two are Bellman equations):\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}\\big[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s\\big]$$\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\mathbb{E}\\big[R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t=a\\big]$$\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\mathbb{E}\\big[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s, A_t=a\\big]$$\n",
    "\n",
    "#### Remark\n",
    "It is important to note that policy $\\pi$ is fixed and doesn't change with time. Although in real practice it is not true, because our strategy is constantly updating when agent learns environment.\n",
    "Policy $\\pi^{*}$ is optimal, if for each state $s$ and any other policy $\\pi$ it is true that $V_{\\pi^{*}}(s) \\geq V_{\\pi}(s)$. Posibly, there are several different optimal policies, however functions $V(s)$ and $Q(s, a)$, which they defines are the same. So, we denote these functions $V^*(s)$ и $Q^*(s, a)$ as optimal.\n",
    "\n",
    "Equations below are Bellman optimality equations (more info [here](https://en.wikipedia.org/wiki/Bellman_equation):\n",
    "$$V^*(s) = \\max_{a \\in \\mathcal{A}} \\mathbb{E}\\big[R_{t+1} + \\gamma V^*(S_{t+1}) | S_t=s, A_t=a\\big]$$\n",
    "$$Q^*(s, a) = \\mathbb{E}\\big[R_{t+1} + \\gamma \\max_{b \\in \\mathcal{A}} Q^*(S_{t+1}, b) | S_t=s, A_t=a\\big]$$\n",
    "\n",
    "Then the following two greedy strategies are equivalent and optimal ($\\mathcal{A}_t$ – set):\n",
    "$$\\mathcal{A}_t = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E}\\big[R_{t+1} + \\gamma V^*(s_{t+1}) | s_t, a \\big]$$\n",
    "$$\\mathcal{A}_t = \\arg\\max_{a \\in \\mathcal{A}} Q^*(s_t, a)$$\n",
    "\n",
    "#### Dynamic programming\n",
    "This part is based on the fact, that $p(s',r|s,a)$ is known.\n",
    "\n",
    "#### Iterative policy evaluation\n",
    "Let agent has some fixed policy $\\pi$, then for $V^{\\pi}$ we have\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[r + \\gamma V^{\\pi}(s') \\big]$$\n",
    "\n",
    "For simplicity we assume that the distribution of rewards is discrete random variable. Otherwise, it is necessary to use operator of integration instead of sum. For each state $s$ we can can evaluate $V^{\\pi}$, using iterative algorithm ($V_0$ is initialized randomly)\n",
    "$$V_{n+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[r + \\gamma V_n(s') \\big].$$\n",
    "\n",
    "By the way, if $s$ is a final state, then $V(s)$ is already known and we may use this value as initialization for faster convergence. The computation stops when $V(s)$ varies negligible from iteration to iteration.\n",
    "\n",
    "#### Remarks\n",
    "1. We make assumption that $p(s',r|s,a)$ is known, but it's often not truth in practice.\n",
    "2. Compution of $V_{n+1}$ can be done **in place**, it accelerates convergence, since in the process of computation we will use the values from iteration $n$ and some new values from iteration $n+1$.\n",
    "\n",
    "#### Policy improvement theorem\n",
    "Suppose there are policies $\\pi$ and $\\pi'$, and for each state $s$ we have\n",
    "$$Q^{\\pi}(s, \\pi'(s)) \\geq V^{\\pi}(s),$$\n",
    "then the following inequality holds\n",
    "$$V^{\\pi'}(s) \\geq V^{\\pi}(s).$$\n",
    "\n",
    "The main interest of this theorem is that it allows us to improve the current policy. We know how to calculate $V^{\\pi}(s)$, so\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) =\n",
    "\\mathbb{E}\\big[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s, A_t=a\\big] =\n",
    "\\sum_{s',r}p(s',r|s, a)\\big[ r + \\gamma V^{\\pi}(s') \\big]\n",
    "$$\n",
    "\n",
    "In this way we can improve our policy\n",
    "$$\\pi'(a) = \\arg\\max_a Q^{\\pi}(s, a).$$\n",
    "The writing above means, that policy chooses action uniformly among all optimal actions. If we can't improve $V^{\\pi}$, then out state-value function is optimal and satisfies the equation the Bellman equation. So, policy, which defines this policy, is also optimal.\n",
    "\n",
    "#### Policy interation\n",
    "Performing alternately calculation of $V^{\\pi}$ and improving $\\pi$, we can find optimal policy $\\pi^*$. It should be noted that as initialization values of $V(s)$ you may use state-value function from previous iteration, it helps significantly speeds up the convergence of the algorithm.\n",
    "\n",
    "#### Value interation\n",
    "There is also interesting approach to compute $V^{*}(s)$ of optimal strategy $\\pi^{*}$\n",
    "$$V_{n+1}(s) = \\max_a \\sum_{s',r} p(s',r|s,a) \\big[r + \\gamma V_n(s') \\big].$$\n",
    "\n",
    "You may understand this method in the following way. On the one hand we in greedy manner update state value function during each iteration of value function evaluation. On the other hand we use Bellman equation to evaluate $V^{*}$ iteratively. It's rather practical method.\n",
    "\n",
    "#### Exercises\n",
    "1. Let you environment have 100 states. The distribution function $p(s',r|s,a)$ may be defined by your own.\n",
    "2. Fix some discount value and find optimal policy.\n",
    "3. What method is faster? For any state $s$, check how $V^*(s)$ varies during $\\gamma$ changing.\n",
    "4. Plot $\\gamma \\times V^*(s)$ for some 5 states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Model problem\n",
    "As a model problem we choose tic-tac-toe game. One agent plays against another, so environament for each player is its opponent. The set of states $\\mathcal{S}_{X}$ is all possible positions of tic-tac-toe game for Xs and the set $\\mathcal{S}_{O}$ defined in the same way for Os. You need to train both agents during some series of games where they play with each other. The criterion of success – both agent don't lose human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Temporal difference\n",
    "The speech will focus on more practical methods. Let imagine that we evaluate $V^{\\pi}(s)$ for our policy $\\pi$. Our agent visited state $s_t$ and we know total reward $G_t$. When agent visites the state $s_t$, we can improve estimation of $V^{\\pi}$ using exponential smoothing\n",
    "$$V^{\\pi}_{n+1}(s_t) = V_{n}^{\\pi}(s_t) + \\alpha \\big(G_t - V_{n}^{\\pi}(s_t)\\big).$$\n",
    "\n",
    "However, you can make some trick and rewrite $G_t$\n",
    "$$V_{n+1}^{\\pi}(s_t) = V_{n}^{\\pi}(s_t) + \\alpha \\big(R_t + \\gamma V^{\\pi}(S_{t+1}) - V_{n}^{\\pi}(s_t)\\big),$$\n",
    "so we can update $V^{\\pi}(s_t)$ after each visiting $s_t$. Similarly, we can compute\n",
    "$$Q_{n+1}(s_t, a_t) = Q_{n}(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) - Q_{n}^{\\pi}(s_t, a_t)\\big).$$\n",
    "\n",
    "#### SARSA (State-Action-Reward-State-Action)\n",
    "So, we want to find optimal policy $\\pi^{*}$. Let's suppose we have some initial approximation $Q(s, a)$. We use нее $\\varepsilon$-greedy policy $\\mathcal{A}_s = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)$. \n",
    "\n",
    "$$\\pi(a|s)=\\frac{1-\\varepsilon}{|\\mathcal{A}_s|}[a \\in \\mathcal{A}_s] + \\frac{\\varepsilon}{|\\mathcal{A}|},\\ \\varepsilon \\in [0, 1].$$\n",
    "\n",
    "In accordance with policy $\\pi$ we choose $a_t$ and get reward $R_{t+1}$, after that environment move agent to state $S_{t+1}$. Agent one more time choose action $A_{t+1} \\sim \\pi(a|S_{t+1})$ and doesn't act in the environemnt, but only update $Q$ function\n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) - Q^{\\pi}(s_t, a_t)\\big).$$\n",
    "\n",
    "Also, there is more variation of this idea, that uses expected Q-value.\n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma \\sum_b \\pi(b|S_{t+1}) Q^{\\pi}(S_{t+1}, b) - Q^{\\pi}(s_t, a_t)\\big).$$\n",
    "So we use $\\varepsilon$-greedy approach to improve our policy and evaluate.\n",
    "\n",
    "####  Q-learning\n",
    "The idea of this method is rather similar, but we also greedy update action-value function $Q$ \n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\big(R_{t+1} + \\gamma \\max_{b} Q(S_{t+1}, b) - Q(s_t, a_t)\\big).$$ For choosing actions during learning agent can use any policy, e.g. $\\varepsilon$-greedy or softmax policy.\n",
    "\n",
    "#### Exercises\n",
    "1. Implement both algorithm for our model problem.\n",
    "2. Which one converges faster? How can you estimate it?\n",
    "3. Compare how action-value function $Q(s_t, a_t)$ differs for these two methods.\n",
    "4. Try to play with your agents...\n",
    "5. Use intentionally losing strategy for Os. What happened?\n",
    "6. Try to beat the agent for Xs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
